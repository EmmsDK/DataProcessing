{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install aiohttp\n",
    "#1 time run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6625eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871faa30",
   "metadata": {},
   "source": [
    "# All possible ranks defined by tiers and divisions\n",
    "# \"pages\" is the first 2 pages of ranked players in each tier and division\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66417e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiers=[\"IRON\",\"BRONZE\",\"SILVER\",\"GOLD\",\"PLATINUM\",\"EMERALD\",\"DIAMOND\"]\n",
    "divisions = [\"I\", \"II\", \"III\", \"IV\"]\n",
    "pages=[\"1\",\"2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f8818",
   "metadata": {},
   "source": [
    "fetch_and_save_ranked_players loops through all possible combinations of tiers combined with divisions and fetches the first 2 pages. It is just called using the previously instantiated arrays.\n",
    "\n",
    "It should only be used if a big sample of players is needed.\n",
    "The api_key is currently the personal API key from LeaguesDeveloper and will most likely need to be updated.\n",
    "\n",
    "In the base_url also contains a region parameter, currently kr for korea. Changed it to match desired region\n",
    "\n",
    "Finally, file_path defines the desire location for saving and storing all of the summonernames of the players in a csv file. It also saved tier and division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b42421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_ranked_players(tiers, divisions, pages):\n",
    "    api_key = \"RGAPI-8840c4e8-ef3d-40cb-90fa-2ea005f2bb1c\"  # Use your actual API key\n",
    "    base_url = \"https://kr.api.riotgames.com/lol/league/v4/entries/RANKED_SOLO_5x5/{tier}/{division}?page={page}\"\n",
    "    headers = {\n",
    "        \"X-Riot-Token\": api_key\n",
    "    }\n",
    "    \n",
    "    file_path = '../DataProcessing/TestData/rankedPlayersKorea.csv'\n",
    "    \n",
    "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['tier', 'division', 'summonerName'])  # CSV Header\n",
    "\n",
    "        for tier in tiers:\n",
    "            for division in divisions:\n",
    "                for page in pages:\n",
    "                    url = base_url.format(tier=tier, division=division, page=page)\n",
    "                    response = requests.get(url, headers=headers)\n",
    "\n",
    "                    # Wait for 2 seconds before making the next request\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    if response.status_code == 200:\n",
    "                        players = response.json()\n",
    "                        for player in players:\n",
    "                            if player['wins'] + player['losses'] > 20:\n",
    "                                writer.writerow([player['tier'], player['rank'], player['summonerName']])\n",
    "                    else:\n",
    "                        print(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c2549",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c82a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN IF NEW DATA REQUIRED\n",
    "#made with personal apikey, therefore following personal pullrequest limitation\n",
    "#fetch_and_save_ranked_players(tiers, divisions, pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a39ff",
   "metadata": {},
   "source": [
    "find_duplicate_summoner_names_with_rows finds the dublicate summonernames from previous function, not editing anything. Should use same file_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d848660e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate: 미라클제이티 found in rows: [67, 70]\n",
      "Duplicate: KoreaJy found in rows: [68, 71]\n",
      "Duplicate: 비챤 팬인 고라니 found in rows: [69, 72]\n",
      "Duplicate: 무적대봉 found in rows: [225, 228]\n",
      "Duplicate: Mute All Live found in rows: [226, 229]\n",
      "Duplicate: 아개졸 found in rows: [227, 230]\n",
      "Duplicate: 핸젤과그랬디야 found in rows: [381, 384]\n",
      "Duplicate: 100363476del found in rows: [382, 385]\n",
      "Duplicate: 오리새뀌 found in rows: [383, 386]\n",
      "Duplicate: 삐뚤어진 또치 found in rows: [632, 634]\n",
      "Duplicate: 껍질벗기기기장인 found in rows: [633, 635]\n",
      "Duplicate: 기운찬곰텡 found in rows: [869, 874]\n",
      "Duplicate: 미드보짓군영 found in rows: [870, 875]\n",
      "Duplicate: Future maker KR found in rows: [871, 876]\n",
      "Duplicate: 응급실카사노바 found in rows: [872, 877]\n",
      "Duplicate: PAKACHU6 found in rows: [873, 878]\n",
      "Duplicate: 백만송이라벤더 found in rows: [1071, 1073]\n",
      "Duplicate: 꾸그밍 found in rows: [1072, 1074]\n",
      "Duplicate: 간티드 found in rows: [1267, 1268]\n",
      "Duplicate: XX황족미드XX found in rows: [1469, 1471]\n",
      "Duplicate: 피키포킹 found in rows: [1470, 1472]\n",
      "Duplicate: 우주의치킨 found in rows: [1679, 1683]\n",
      "Duplicate: 민짜의속도위반 found in rows: [1680, 1684]\n",
      "Duplicate: 아이언급행이다 found in rows: [1681, 1685]\n",
      "Duplicate: 겜 달 found in rows: [1682, 1686]\n",
      "Duplicate: 건더기장군 found in rows: [1908, 1911]\n",
      "Duplicate: 아오이 사키 found in rows: [1909, 1912]\n",
      "Duplicate: 흐헉소환사라능 found in rows: [1910, 1913]\n",
      "Duplicate: 원숭이바위 found in rows: [2141, 2143]\n",
      "Duplicate: 참된학생바른학생 found in rows: [2142, 2144]\n",
      "Duplicate: 강해린은내꺼 found in rows: [2376, 2378]\n",
      "Duplicate: 포솜포솜 found in rows: [2377, 2379]\n",
      "Duplicate: 화려한 네임드 found in rows: [2600, 2603]\n",
      "Duplicate: 엄한결 found in rows: [2601, 2604]\n",
      "Duplicate: 즐겜유저장혜지 found in rows: [2602, 2605]\n",
      "Duplicate: 신월동 빵댕이 found in rows: [2784, 2786]\n",
      "Duplicate: 100160679del found in rows: [2785, 2787]\n",
      "Duplicate: 오른누나아칼리 found in rows: [3006, 3007]\n",
      "Duplicate: Anno Domini AD found in rows: [3229, 3231]\n",
      "Duplicate: Best 우주여신 found in rows: [3230, 3232]\n",
      "Duplicate: 내년에롤접음 found in rows: [3457, 3459]\n",
      "Duplicate: Hyungseokgaesae found in rows: [3458, 3460]\n",
      "Duplicate: 무조건탑은점화 found in rows: [3697, 3702]\n",
      "Duplicate: Sea8 found in rows: [3698, 3703]\n",
      "Duplicate: 나른한상상 found in rows: [3699, 3704]\n",
      "Duplicate: vcafd found in rows: [3700, 3705]\n",
      "Duplicate: 2006 01 19 found in rows: [3701, 3706]\n",
      "Duplicate: mi sheou maing found in rows: [3954, 3957]\n",
      "Duplicate: 06 이승민 found in rows: [3955, 3958]\n",
      "Duplicate: 100152825del found in rows: [3956, 3959]\n",
      "Duplicate: 10sus found in rows: [4194, 4197]\n",
      "Duplicate: gangbros found in rows: [4195, 4198]\n",
      "Duplicate: ZUNZZI found in rows: [4196, 4199]\n",
      "Duplicate: 나의협곡일기 found in rows: [4441, 4444]\n",
      "Duplicate: 아중리이프로 found in rows: [4442, 4445]\n",
      "Duplicate: 미드가고싶은날 found in rows: [4443, 4446]\n",
      "Duplicate: 팀운개지리네 0 found in rows: [4714, 4717]\n",
      "Duplicate: 다이아용 부캐 found in rows: [4715, 4718]\n",
      "Duplicate: 디현이 found in rows: [4716, 4719]\n",
      "Duplicate: 산책하는마루 found in rows: [5000, 5005]\n",
      "Duplicate: 3chris found in rows: [5001, 5006]\n",
      "Duplicate: 승겸갓저격수 found in rows: [5002, 5007]\n",
      "Duplicate: 1234보 found in rows: [5003, 5008]\n",
      "Duplicate: nahgnues found in rows: [5004, 5009]\n",
      "Duplicate: 대지에 축복을 found in rows: [5295, 5299]\n",
      "Duplicate: 나라코알 found in rows: [5296, 5300]\n",
      "Duplicate: TheCollaboration found in rows: [5297, 5301]\n",
      "Duplicate: 티스푼입니다만 found in rows: [5298, 5302]\n",
      "Duplicate: 심 장 저 격 중 found in rows: [5638, 5644]\n",
      "Duplicate: 경몽어스 found in rows: [5639, 5645]\n",
      "Duplicate: 늑대의 정석 found in rows: [5640, 5646]\n",
      "Duplicate: His KaiSa found in rows: [5641, 5647]\n",
      "Duplicate: 라 더 found in rows: [5642, 5648]\n",
      "Duplicate: 지나간슬픔에새눈물을낭비하지말자 found in rows: [5643, 5649]\n",
      "Duplicate: Zhaozian found in rows: [6034, 6039]\n",
      "Duplicate: 하 덕 found in rows: [6035, 6040]\n",
      "Duplicate: Zisi de jiyin found in rows: [6036, 6041]\n",
      "Duplicate: Fender USA found in rows: [6037, 6042]\n",
      "Duplicate: GEN Suger found in rows: [6038, 6043]\n",
      "Duplicate: 이수행 여친 도둑 found in rows: [6400, 6403]\n",
      "Duplicate: 레드보리 found in rows: [6401, 6404]\n",
      "Duplicate: ililillilllilill found in rows: [6402, 6405]\n",
      "Duplicate: 룰루보다잘웃어요 found in rows: [6731, 6734]\n",
      "Duplicate: 18TP found in rows: [6732, 6735]\n",
      "Duplicate: zhengxianyu found in rows: [6733, 6736]\n"
     ]
    }
   ],
   "source": [
    "#function for finding dublicates\n",
    "\n",
    "def find_duplicate_summoner_names_with_rows(file_path):\n",
    "    summoner_occurrences = {}  # Tracks summoner names and their row numbers\n",
    "    duplicates_info = []  # Stores information about duplicates\n",
    "\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row_number, row in enumerate(reader, start=1):  # Start counting rows from 1\n",
    "            summoner_name = row['summonerName']\n",
    "            if summoner_name in summoner_occurrences:\n",
    "                # If the summoner name is already encountered, add the current row as a duplicate\n",
    "                summoner_occurrences[summoner_name].append(row_number)\n",
    "            else:\n",
    "                # Otherwise, initialize the list with the current row number\n",
    "                summoner_occurrences[summoner_name] = [row_number]\n",
    "\n",
    "    # Filter out summoner names with more than one occurrence and prepare duplicate info\n",
    "    for summoner_name, row_numbers in summoner_occurrences.items():\n",
    "        if len(row_numbers) > 1:\n",
    "            duplicates_info.append((summoner_name, row_numbers))\n",
    "\n",
    "    return duplicates_info\n",
    "\n",
    "# Specify the file path\n",
    "file_path = '../DataProcessing/TestData/rankedPlayersKorea.csv'\n",
    "duplicates = find_duplicate_summoner_names_with_rows(file_path)\n",
    "\n",
    "# Example output\n",
    "for summoner_name, rows in duplicates:\n",
    "    print(f\"Duplicate: {summoner_name} found in rows: {rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92ef0f",
   "metadata": {},
   "source": [
    "remove_duplicate_summoner_names removes the duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d303e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for removing dublicates\n",
    "\n",
    "def remove_duplicate_summoner_names(file_path):\n",
    "    processed_summoners = set()  # To track unique summonerNames\n",
    "    unique_rows = []  # To store rows after removing duplicates\n",
    "\n",
    "    # Read the file and filter out duplicate summonerNames\n",
    "    with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            if row['summonerName'] not in processed_summoners:\n",
    "                processed_summoners.add(row['summonerName'])\n",
    "                unique_rows.append(row)\n",
    "\n",
    "    # Write the unique rows back to the file\n",
    "    with open(file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        fieldnames = ['tier', 'division', 'summonerName']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique_rows)\n",
    "\n",
    "# Specify the file path\n",
    "#file_path = '../DataProcessing/TestData/rankedPlayers.csv'\n",
    "#remove_duplicate_summoner_names(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419b4e7",
   "metadata": {},
   "source": [
    "The following functions \"split_csv_by_xxx_tier\", separates all the ranked tiers into tier specific csv files.\n",
    "The \"source_file_path\" the functions takes as a parameter is the same file from earlier. It should be the cleaned up version of the summonernames.\n",
    "\"file_name\" and \"file_path\" is the desired name and location of the new file created, only containing the tier specific function.\n",
    "Each of the following 7 functions can be called seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_by_iron_tier(source_file_path):\n",
    "    # Define the base folder for the new files\n",
    "    base_folder = os.path.dirname(source_file_path)\n",
    "    \n",
    "    # Initialize a dictionary to keep track of the CSV writer for the IRON tier\n",
    "    writer = None\n",
    "    file = None\n",
    "    \n",
    "    with open(source_file_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            tier = row['tier']\n",
    "            \n",
    "            # Proceed only if the tier is IRON\n",
    "            if tier == \"IRON\":\n",
    "                file_name = \"IRON_players_korea.csv\"\n",
    "                file_path = os.path.join(base_folder, file_name)\n",
    "                # alternative file_path='../DataProcessing/TestData/IRON_players_korea.csv\n",
    "                \n",
    "                # Check if we already have a writer for the IRON tier, if not, create it\n",
    "                if writer is None:\n",
    "                    file = open(file_path, mode='w', newline='', encoding='utf-8')\n",
    "                    writer = csv.DictWriter(file, fieldnames=reader.fieldnames)\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Write the row to the IRON file\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    # Close the file for the IRON tier if it's open\n",
    "    if file is not None:\n",
    "        file.close()\n",
    "\n",
    "# Specify the source file path\n",
    "source_file_path = '../DataProcessing/TestData/rankedPlayersKorea.csv'\n",
    "\n",
    "# Call the function\n",
    "split_csv_by_iron_tier(source_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_by_bronze_tier(source_file_path):\n",
    "    # Define the base folder for the new files\n",
    "    base_folder = os.path.dirname(source_file_path)\n",
    "    \n",
    "    # Initialize a dictionary to keep track of the CSV writer for the BRONZE tier\n",
    "    writer = None\n",
    "    file = None\n",
    "    \n",
    "    with open(source_file_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            tier = row['tier']\n",
    "            \n",
    "            # Proceed only if the tier is BRONZE\n",
    "            if tier == \"BRONZE\":\n",
    "                file_name = \"BRONZE_players_korea.csv\"\n",
    "                file_path = os.path.join(base_folder, file_name)\n",
    "                \n",
    "                # Check if we already have a writer for the BRONZE tier, if not, create it\n",
    "                if writer is None:\n",
    "                    file = open(file_path, mode='w', newline='', encoding='utf-8')\n",
    "                    writer = csv.DictWriter(file, fieldnames=reader.fieldnames)\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Write the row to the BRONZE file\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    # Close the file for the BRONZE tier if it's open\n",
    "    if file is not None:\n",
    "        file.close()\n",
    "\n",
    "# Specify the source file path\n",
    "source_file_path = '../DataProcessing/TestData/rankedPlayersKorea.csv'\n",
    "\n",
    "# Call the function\n",
    "split_csv_by_bronze_tier(source_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_by_silver_tier(source_file_path):\n",
    "    # Define the base folder for the new files\n",
    "    base_folder = os.path.dirname(source_file_path)\n",
    "    \n",
    "    # Initialize a dictionary to keep track of the CSV writer for the SILVER tier\n",
    "    writer = None\n",
    "    file = None\n",
    "    \n",
    "    with open(source_file_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            tier = row['tier']\n",
    "            \n",
    "            # Proceed only if the tier is SILVER\n",
    "            if tier == \"SILVER\":\n",
    "                file_name = \"SILVER_players_korea.csv\"\n",
    "                file_path = os.path.join(base_folder, file_name)\n",
    "                \n",
    "                # Check if we already have a writer for the SILVER tier, if not, create it\n",
    "                if writer is None:\n",
    "                    file = open(file_path, mode='w', newline='', encoding='utf-8')\n",
    "                    writer = csv.DictWriter(file, fieldnames=reader.fieldnames)\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Write the row to the SILVER file\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    # Close the file for the SILVER tier if it's open\n",
    "    if file is not None:\n",
    "        file.close()\n",
    "\n",
    "# Specify the source file path\n",
    "source_file_path = '../DataProcessing/TestData/rankedPlayersKorea.csv'\n",
    "\n",
    "# Call the function\n",
    "split_csv_by_silver_tier(source_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_by_gold_tier(source_file_path):\n",
    "    # Define the base folder for the new files\n",
    "    base_folder = os.path.dirname(source_file_path)\n",
    "    \n",
    "    # Initialize a dictionary to keep track of the CSV writer for the GOLD tier\n",
    "    writer = None\n",
    "    file = None\n",
    "    \n",
    "    with open(source_file_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            tier = row['tier']\n",
    "            \n",
    "            # Proceed only if the tier is GOLD\n",
    "            if tier == \"GOLD\":\n",
    "                file_name = \"GOLD_players_korea.csv\"\n",
    "                file_path = os.path.join(base_folder, file_name)\n",
    "                \n",
    "                # Check if we already have a writer for the GOLD tier, if not, create it\n",
    "                if writer is None:\n",
    "                    file = open(file_path, mode='w', newline='', encoding='utf-8')\n",
    "                    writer = csv.DictWriter(file, fieldnames=reader.fieldnames)\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Write the row to the GOLD file\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    # Close the file for the GOLD tier if it's open\n",
    "    if file is not None:\n",
    "        file.close()\n",
    "\n",
    "# Specify the source file path\n",
    "source_file_path = '../DataProcessing/TestData/rankedPlayersKorea.csv'\n",
    "\n",
    "# Call the function\n",
    "split_csv_by_gold_tier(source_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb290674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_by_platinum_tier(source_file_path):\n",
    "    # Define the base folder for the new files\n",
    "    base_folder = os.path.dirname(source_file_path)\n",
    "    \n",
    "    # Initialize a dictionary to keep track of the CSV writer for the PLATINUM tier\n",
    "    writer = None\n",
    "    file = None\n",
    "    \n",
    "    with open(source_file_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            tier = row['tier']\n",
    "            \n",
    "            # Proceed only if the tier is PLATINUM\n",
    "            if tier == \"PLATINUM\":\n",
    "                file_name = \"PLATINUM_players_korea.csv\"\n",
    "                file_path = os.path.join(base_folder, file_name)\n",
    "                \n",
    "                # Check if we already have a writer for the PLATINUM tier, if not, create it\n",
    "                if writer is None:\n",
    "                    file = open(file_path, mode='w', newline='', encoding='utf-8')\n",
    "                    writer = csv.DictWriter(file, fieldnames=reader.fieldnames)\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Write the row to the PLATINUM file\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    # Close the file for the PLATINUM tier if it's open\n",
    "    if file is not None:\n",
    "        file.close()\n",
    "\n",
    "# Specify the source file path\n",
    "source_file_path = '../DataProcessing/TestData/rankedPlayersKorea.csv'\n",
    "\n",
    "# Call the function\n",
    "split_csv_by_platinum_tier(source_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74df7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_by_emerald_tier(source_file_path):\n",
    "    # Define the base folder for the new files\n",
    "    base_folder = os.path.dirname(source_file_path)\n",
    "    \n",
    "    # Initialize a dictionary to keep track of the CSV writer for the EMERALD tier\n",
    "    writer = None\n",
    "    file = None\n",
    "    \n",
    "    with open(source_file_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            tier = row['tier']\n",
    "            \n",
    "            # Proceed only if the tier is EMERALD\n",
    "            if tier == \"EMERALD\":\n",
    "                file_name = \"EMERALD_players_korea.csv\"\n",
    "                file_path = os.path.join(base_folder, file_name)\n",
    "                \n",
    "                # Check if we already have a writer for the EMERALD tier, if not, create it\n",
    "                if writer is None:\n",
    "                    file = open(file_path, mode='w', newline='', encoding='utf-8')\n",
    "                    writer = csv.DictWriter(file, fieldnames=reader.fieldnames)\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Write the row to the EMERALD file\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    # Close the file for the EMERALD tier if it's open\n",
    "    if file is not None:\n",
    "        file.close()\n",
    "\n",
    "# Specify the source file path\n",
    "source_file_path = '../DataProcessing/TestData/rankedPlayersKorea.csv'\n",
    "\n",
    "# Call the function\n",
    "split_csv_by_emerald_tier(source_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58ba4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_by_diamond_tier(source_file_path):\n",
    "    # Define the base folder for the new files\n",
    "    base_folder = os.path.dirname(source_file_path)\n",
    "    \n",
    "    # Initialize a dictionary to keep track of the CSV writer for the DIAMOND tier\n",
    "    writer = None\n",
    "    file = None\n",
    "    \n",
    "    with open(source_file_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            tier = row['tier']\n",
    "            \n",
    "            # Proceed only if the tier is DIAMOND\n",
    "            if tier == \"DIAMOND\":\n",
    "                file_name = \"DIAMOND_players_korea.csv\"\n",
    "                file_path = os.path.join(base_folder, file_name)\n",
    "                \n",
    "                # Check if we already have a writer for the DIAMOND tier, if not, create it\n",
    "                if writer is None:\n",
    "                    file = open(file_path, mode='w', newline='', encoding='utf-8')\n",
    "                    writer = csv.DictWriter(file, fieldnames=reader.fieldnames)\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Write the row to the DIAMOND file\n",
    "                writer.writerow(row)\n",
    "    \n",
    "    # Close the file for the DIAMOND tier if it's open\n",
    "    if file is not None:\n",
    "        file.close()\n",
    "\n",
    "# Specify the source file path\n",
    "source_file_path = '../DataProcessing/TestData/rankedPlayersKorea.csv'\n",
    "\n",
    "# Call the function\n",
    "split_csv_by_diamond_tier(source_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474df2f",
   "metadata": {},
   "source": [
    "get_summoner_names_from_csv loads summonernames into an array for futher processing. file_path is the path to one of the files previously created with \"split_csv_by_xxx_tier\"\n",
    "The code cell below storages the names in the variable \"summoner_names\" which will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48658ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# saves summonernames for further processiing\n",
    "def get_summoner_names_from_csv(file_path):\n",
    "    summoner_names = []\n",
    "    \n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            summoner_names.append(row['summonerName'])\n",
    "    \n",
    "    return summoner_names\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = '../DataProcessing/TestData/DIAMOND_players_korea.csv'\n",
    "summoner_names = get_summoner_names_from_csv(file_path)\n",
    "print(summoner_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0445a8",
   "metadata": {},
   "source": [
    "\"update_summoner_names_with_puuid\" replaces the summonernames with puuid, which can be used for finding ranked games later\n",
    "\n",
    "replace api_key with your apikey, and be aware of the url. It should be changed to desired region. The relevant ones is kr and eu\n",
    "\n",
    "summoner_names is being used here, with the same filepath used to generate summoner_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f873ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#personal apikey function, replacing summonernames with puuid\n",
    "#FINDS PUUID FROM SUMMONER NAME\n",
    "def update_summoner_names_with_puuid(file_path, summoner_names):\n",
    "    api_key = \"your key\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"da-DK,da;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"Accept-Charset\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "        \"Origin\": \"https://developer.riotgames.com\",\n",
    "        \"X-Riot-Token\": api_key\n",
    "    }\n",
    "    summoner_to_puuid = {}\n",
    "\n",
    "    start_time = time.time()\n",
    "    last_update_time = start_time\n",
    "    total_names = len(summoner_names)\n",
    "    names_processed = 0\n",
    "\n",
    "    # Fetch puuid for each summoner name\n",
    "    for name in summoner_names:\n",
    "        url = f\"https://kr.api.riotgames.com/lol/summoner/v4/summoners/by-name/{name}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            summoner_to_puuid[name] = data['puuid']\n",
    "        else:\n",
    "            print(f\"Error fetching data for {name}: {response.status_code}\")\n",
    "        names_processed += 1\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Check if 1 minute has passed for progress update\n",
    "        if current_time - last_update_time >= 60:\n",
    "            print(f\"Progress: {names_processed}/{total_names} summoner names processed.\")\n",
    "            last_update_time = current_time\n",
    "\n",
    "        time.sleep(1.5)  # Delay to comply with rate limit\n",
    "    \n",
    "    # Update progress after finishing all requests\n",
    "    print(f\"Progress: {names_processed}/{total_names} summoner names processed. Update complete.\")\n",
    "\n",
    "    # Now, replace summoner names with puuid in the CSV\n",
    "    temp_file_path = file_path + \".tmp\"\n",
    "    with open(file_path, mode='r', encoding='utf-8') as infile, open(temp_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in reader:\n",
    "            if row['summonerName'] in summoner_to_puuid:\n",
    "                row['summonerName'] = summoner_to_puuid[row['summonerName']]\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    # Replace original file with the updated temp file\n",
    "    os.replace(temp_file_path, file_path)\n",
    "\n",
    "\n",
    "# Call the function\n",
    "update_summoner_names_with_puuid(file_path, summoner_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c6a3f",
   "metadata": {},
   "source": [
    "\"fetch_matches_and_save\" fetches the last 20 matches if possible from each of the puuids.\n",
    "\n",
    "edit url from asia to europe and vice versa if necessary\n",
    "\n",
    "parameters should be the previous file path for the file containing all the puuids and the path for desired csv file that should contain the matchids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2be18695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 21 summoners processed.\n",
      "Progress: 41 summoners processed.\n",
      "Progress: 61 summoners processed.\n",
      "Progress: 81 summoners processed.\n",
      "Progress: 101 summoners processed.\n",
      "Progress: 121 summoners processed.\n",
      "Progress: 141 summoners processed.\n",
      "Progress: 161 summoners processed.\n",
      "Progress: 181 summoners processed.\n",
      "Progress: 201 summoners processed.\n",
      "Progress: 221 summoners processed.\n",
      "Progress: 241 summoners processed.\n",
      "Progress: 261 summoners processed.\n",
      "Progress: 281 summoners processed.\n",
      "Progress: 301 summoners processed.\n",
      "Progress: 321 summoners processed.\n",
      "Progress: 341 summoners processed.\n",
      "Progress: 361 summoners processed.\n",
      "Progress: 381 summoners processed.\n",
      "Progress: 401 summoners processed.\n",
      "Progress: 421 summoners processed.\n",
      "Progress: 441 summoners processed.\n",
      "Progress: 461 summoners processed.\n",
      "Progress: 481 summoners processed.\n",
      "Progress: 501 summoners processed.\n",
      "Progress: 521 summoners processed.\n",
      "Progress: 541 summoners processed.\n",
      "Progress: 561 summoners processed.\n",
      "Progress: 581 summoners processed.\n",
      "Progress: 601 summoners processed.\n",
      "Progress: 621 summoners processed.\n",
      "Progress: 641 summoners processed.\n",
      "Progress: 661 summoners processed.\n",
      "Progress: 681 summoners processed.\n",
      "Progress: 701 summoners processed.\n",
      "Progress: 721 summoners processed.\n",
      "Progress: 741 summoners processed.\n",
      "Progress: 761 summoners processed.\n",
      "Progress: 781 summoners processed.\n",
      "Progress: 801 summoners processed.\n",
      "Progress: 821 summoners processed.\n",
      "Progress: 841 summoners processed.\n",
      "Progress: 861 summoners processed.\n",
      "Progress: 881 summoners processed.\n",
      "Progress: 901 summoners processed.\n",
      "Progress: 921 summoners processed.\n",
      "Final Progress: 921 summoners processed. Task complete.\n",
      "Progress: 21 summoners processed.\n",
      "Progress: 41 summoners processed.\n",
      "Progress: 61 summoners processed.\n",
      "Progress: 81 summoners processed.\n",
      "Progress: 101 summoners processed.\n",
      "Progress: 121 summoners processed.\n",
      "Progress: 141 summoners processed.\n",
      "Progress: 161 summoners processed.\n",
      "Progress: 181 summoners processed.\n",
      "Progress: 201 summoners processed.\n",
      "Progress: 221 summoners processed.\n",
      "Progress: 241 summoners processed.\n",
      "Progress: 261 summoners processed.\n",
      "Progress: 281 summoners processed.\n",
      "Progress: 301 summoners processed.\n",
      "Progress: 321 summoners processed.\n",
      "Progress: 341 summoners processed.\n",
      "Progress: 361 summoners processed.\n",
      "Progress: 381 summoners processed.\n",
      "Progress: 401 summoners processed.\n",
      "Progress: 421 summoners processed.\n",
      "Progress: 441 summoners processed.\n",
      "Progress: 461 summoners processed.\n",
      "Progress: 481 summoners processed.\n",
      "Progress: 501 summoners processed.\n",
      "Progress: 521 summoners processed.\n",
      "Progress: 541 summoners processed.\n",
      "Progress: 561 summoners processed.\n",
      "Progress: 581 summoners processed.\n",
      "Progress: 601 summoners processed.\n",
      "Progress: 621 summoners processed.\n",
      "Progress: 641 summoners processed.\n",
      "Progress: 661 summoners processed.\n",
      "Progress: 681 summoners processed.\n",
      "Progress: 701 summoners processed.\n",
      "Progress: 721 summoners processed.\n",
      "Progress: 741 summoners processed.\n",
      "Progress: 761 summoners processed.\n",
      "Final Progress: 774 summoners processed. Task complete.\n",
      "Progress: 21 summoners processed.\n",
      "Progress: 41 summoners processed.\n",
      "Progress: 61 summoners processed.\n",
      "Progress: 81 summoners processed.\n",
      "Progress: 101 summoners processed.\n",
      "Progress: 121 summoners processed.\n",
      "Progress: 141 summoners processed.\n",
      "Progress: 161 summoners processed.\n",
      "Progress: 181 summoners processed.\n",
      "Progress: 201 summoners processed.\n",
      "Progress: 221 summoners processed.\n",
      "Progress: 241 summoners processed.\n",
      "Progress: 261 summoners processed.\n",
      "Progress: 281 summoners processed.\n",
      "Progress: 301 summoners processed.\n",
      "Progress: 321 summoners processed.\n",
      "Progress: 341 summoners processed.\n",
      "Progress: 361 summoners processed.\n",
      "Progress: 381 summoners processed.\n",
      "Progress: 401 summoners processed.\n",
      "Progress: 421 summoners processed.\n",
      "Progress: 441 summoners processed.\n",
      "Progress: 461 summoners processed.\n",
      "Progress: 481 summoners processed.\n",
      "Progress: 501 summoners processed.\n",
      "Progress: 521 summoners processed.\n",
      "Progress: 541 summoners processed.\n",
      "Progress: 561 summoners processed.\n",
      "Progress: 581 summoners processed.\n",
      "Progress: 601 summoners processed.\n",
      "Progress: 621 summoners processed.\n",
      "Progress: 641 summoners processed.\n",
      "Progress: 661 summoners processed.\n",
      "Progress: 681 summoners processed.\n",
      "Progress: 701 summoners processed.\n",
      "Progress: 721 summoners processed.\n",
      "Progress: 741 summoners processed.\n",
      "Progress: 761 summoners processed.\n",
      "Final Progress: 762 summoners processed. Task complete.\n"
     ]
    }
   ],
   "source": [
    "def fetch_matches_and_save(source_csv_path, target_csv_path):\n",
    "    api_key = \"your apikey\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"da-DK,da;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"Accept-Charset\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "        \"Origin\": \"https://developer.riotgames.com\",\n",
    "        \"X-Riot-Token\": api_key\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    last_update_time = start_time\n",
    "    summoners_processed = 0\n",
    "\n",
    "    # Ensure the target file directory exists\n",
    "    os.makedirs(os.path.dirname(target_csv_path), exist_ok=True)\n",
    "\n",
    "    with open(source_csv_path, mode='r', encoding='utf-8') as infile, open(target_csv_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        csv_reader = csv.DictReader(infile)\n",
    "        csv_writer = csv.writer(outfile)\n",
    "        csv_writer.writerow(['MatchID'])  # Header for target CSV\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            puuid = row['summonerName']  # Assuming this column actually contains puuids\n",
    "            url = f\"https://asia.api.riotgames.com/lol/match/v5/matches/by-puuid/{puuid}/ids?type=ranked&start=0&count=20\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                match_ids = response.json()\n",
    "                for match_id in match_ids:\n",
    "                    csv_writer.writerow([match_id])\n",
    "            else:\n",
    "                print(f\"Error fetching matches for {puuid}: {response.status_code}\")\n",
    "            \n",
    "            summoners_processed += 1\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            # Print progress every minute\n",
    "            if current_time - last_update_time >= 60:\n",
    "                print(f\"Progress: {summoners_processed} summoners processed.\")\n",
    "                last_update_time = current_time\n",
    "\n",
    "    # Final progress update\n",
    "    print(f\"Final Progress: {summoners_processed} summoners processed. Task complete.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "#source_csv_path = '../DataProcessing/TestData/SILVER_players_korea.csv'\n",
    "#target_csv_path = '../DataProcessing/TestData/SILVER_GAMES_KOREA.csv'\n",
    "#fetch_matches_and_save(source_csv_path, target_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf2cb6",
   "metadata": {},
   "source": [
    "def remove_duplicate_matches(csv_file_path, output_file_path):\n",
    "    \n",
    "    Removes duplicate records from a CSV file based on unique MatchID values and saves the cleaned data to a new CSV file.\n",
    "    \n",
    "    This function reads a CSV file into a pandas DataFrame, removes duplicate entries based on the 'MatchID' column by      keeping only the first occurrence of each duplicate, and then saves the cleaned DataFrame to a specified output CSV     file. A confirmation message is printed to indicate the successful saving of the cleaned CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_file_path (str): The file path to the input CSV file containing the original dataset with potential duplicate               matches.\n",
    "    - output_file_path (str): The file path where the cleaned dataset without duplicates will be saved.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9954c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to ../DataProcessing/TestData/CLEANED_IRON_GAMES_KOREA.csv.\n",
      "Cleaned CSV saved to ../DataProcessing/TestData/CLEANED_BRONZE_GAMES_KOREA.csv.\n",
      "Cleaned CSV saved to ../DataProcessing/TestData/CLEANED_SILVER_GAMES_KOREA.csv.\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicate_matches(csv_file_path, output_file_path):\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Remove duplicates based on the 'MatchID' column and keep the first occurrence\n",
    "    cleaned_df = df.drop_duplicates('MatchID', keep='first')\n",
    "    \n",
    "    # Save the cleaned DataFrame to a new CSV file\n",
    "    cleaned_df.to_csv(output_file_path, index=False)\n",
    "    \n",
    "    print(f\"Cleaned CSV saved to {output_file_path}.\")\n",
    "    \n",
    "csv_file_path='../DataProcessing/TestData/IRON_GAMES_KOREA.csv'\n",
    "output_file_path='../DataProcessing/TestData/CLEANED_IRON_GAMES_KOREA.csv'\n",
    "remove_duplicates_and_save(csv_file_path, output_file_path)\n",
    "\n",
    "csv_file_path='../DataProcessing/TestData/BRONZE_GAMES_KOREA.csv'\n",
    "output_file_path='../DataProcessing/TestData/CLEANED_BRONZE_GAMES_KOREA.csv'\n",
    "remove_duplicates_matches(csv_file_path, output_file_path)\n",
    "\n",
    "#source_csv_path = \"../DataProcessing/TestData/GOLD_players.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79300f7f",
   "metadata": {},
   "source": [
    "def call_api_and_save_response_with_progress(csv_file_path, save_folder_path):\n",
    "    \"\"\"\n",
    "    Calls an API for each match ID listed in a CSV file and saves the response to a JSON file in the specified folder, with progress updates.\n",
    "    \n",
    "    This function reads match IDs from a CSV file and makes API calls for each ID to fetch match data. The responses are    saved as JSON files in a specified directory. The function handles rate limiting by ensuring that no more than 30       requests are made per second. It provides progress updates every minute and a final completion message once all matches are processed.\n",
    "    \n",
    "    Parameters:\n",
    "    - continent (str): the target continent of matchids. europe for EUW1_matches for, asia for KR.\n",
    "    - csv_file_path (str): The file path to the input CSV file containing match IDs.\n",
    "    - save_folder_path (str): The directory path where the JSON files with the API responses will be saved.\n",
    "    \n",
    "    The function implements rate limiting and error handling for HTTP response codes, ensuring compliance with the API's     request constraints. Progress updates are printed to the console, indicating the number of matches processed and the    overall completion percentage.\n",
    "    \n",
    "    Returns:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb92202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 40 matches processed. 100% complete.\n"
     ]
    }
   ],
   "source": [
    "def call_timelineapi_and_save_response_with_progress(continent,csv_file_path, save_folder_path):\n",
    "    # Define the base URL and updated headers for the API call\n",
    "    base_url = f\"https://{continent}.api.riotgames.com/lol/match/v5/matches/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"da-DK,da;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"Accept-Charset\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "        \"Origin\": \"https://developer.riotgames.com\",\n",
    "        \"X-Riot-Token\": \"productionkey\"\n",
    "    }\n",
    "    \n",
    "    # Read Match IDs from the CSV file\n",
    "    with open(csv_file_path, mode='r', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        match_ids = [row['MatchID'] for row in csv_reader]\n",
    "    \n",
    "    total_matches = len(match_ids)\n",
    "    matches_processed = 0\n",
    "    start_time = time.time()\n",
    "    last_update_time = start_time\n",
    "    request_times = []  # Track the times of the last 30 requests\n",
    "\n",
    "    # Process each Match ID\n",
    "    for match_id in match_ids:\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Check if we have made 30 requests in the last second\n",
    "        if len(request_times) >= 30:\n",
    "            # Calculate the time to wait before making the next request\n",
    "            time_to_wait = 1 - (current_time - request_times[0])\n",
    "            if time_to_wait > 0:\n",
    "                time.sleep(time_to_wait)\n",
    "        \n",
    "        full_url = base_url + match_id + \"/timeline\"\n",
    "        \n",
    "        # Make the API call\n",
    "        response = requests.get(full_url, headers=headers)\n",
    "        \n",
    "        # Update the request_times list\n",
    "        request_times.append(time.time())\n",
    "        if len(request_times) > 30:\n",
    "            request_times.pop(0)  # Remove the oldest request time\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Save the response body as a JSON file\n",
    "            file_path = os.path.join(save_folder_path, f\"{match_id}.json\")\n",
    "            with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(response.json(), json_file, ensure_ascii=False, indent=4)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for Match ID {match_id}. Status Code: {response.status_code}\")\n",
    "        \n",
    "        matches_processed += 1\n",
    "        \n",
    "        if current_time - last_update_time >= 60:\n",
    "            print(f\"Processed {matches_processed}/{total_matches} matches. {int((matches_processed/total_matches)*100)}% complete.\")\n",
    "            last_update_time = current_time\n",
    "    \n",
    "    # Final update\n",
    "    print(f\"All {total_matches} matches processed. 100% complete.\")\n",
    "\n",
    "sgamespath='E:\\\\DistanceTesting\\\\filenames.csv'\n",
    "jsonfilepaths='E:\\\\DistanceTesting\\\\filenamesv2'\n",
    "continent = 'europe'\n",
    "#continent='asia'\n",
    "call_timelineapi_and_save_response_with_progress(continent,sgamespath, jsonfilepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b5f32",
   "metadata": {},
   "source": [
    "def fetch_and_save_match_data(continent, csv_file_path, output_folder_path):\n",
    "    \"\"\"\n",
    "    Fetches match data from a Riot Games API for a specified continent and saves the response bodies to individual JSON files in an output folder.\n",
    "    \n",
    "    This function iterates through match IDs listed in a provided CSV file, constructs API requests to fetch match data from the Riot Games API for the specified continent, and saves each successful response as a JSON file in the designated output folder. It handles basic rate limiting by pacing the requests and provides periodic progress updates after every 250 requests or upon completing all requests. The function also ensures the output folder exists, creating it if necessary.\n",
    "    \n",
    "    Parameters:\n",
    "    - continent (str): The continent from which to fetch match data (e.g., 'europe', 'asia').\n",
    "    - csv_file_path (str): The file path to the input CSV file containing match IDs.\n",
    "    - output_folder_path (str): The directory path where the output JSON files will be saved.\n",
    "    \n",
    "    The function includes error handling for unsuccessful API requests and ensures compliance with the API's rate limit by sleeping briefly between requests. Progress updates are printed to console to track the advancement of the data fetching process.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a4c13fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for MatchID: MatchID, Status Code: 404\n",
      "Progress: 41/41\n",
      "Finished processing. Total requests: 41.\n"
     ]
    }
   ],
   "source": [
    "#function for calling and saving responsebody from postmatch data api\n",
    "def fetch_and_save_postmatch_data(continent, csv_file_path, output_folder_path):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Charset\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "        \"Origin\": \"https://developer.riotgames.com\",\n",
    "        \"X-Riot-Token\": \"production key\"\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    # Count total MatchIDs in the CSV file\n",
    "    with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        total_ids = sum(1 for row in reader)\n",
    "\n",
    "    # Reset file pointer to start\n",
    "    with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        current_position = 0\n",
    "        for row in reader:\n",
    "            match_id = row[0]\n",
    "            url = f\"https://{continent}.api.riotgames.com/lol/match/v5/matches/{match_id}\"\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "            current_position += 1\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                output_file_path = os.path.join(output_folder_path, f\"{match_id}.json\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "                    outfile.write(response.text)\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for MatchID: {match_id}, Status Code: {response.status_code}\")\n",
    "\n",
    "            # Progress update\n",
    "            if current_position % 250 == 0 or current_position == total_ids:  # Update every 250 requests or on last request\n",
    "                print(f\"Progress: {current_position}/{total_ids}\")\n",
    "\n",
    "            time.sleep(1 / 30)  # Respect the rate limit\n",
    "\n",
    "    print(f\"Finished processing. Total requests: {current_position}.\")\n",
    "    \n",
    "csv_file_path='E:\\\\DistanceTesting\\\\filenames.csv'\n",
    "continent = 'europe'\n",
    "#continent='asia'\n",
    "output_folder_path='E:\\\\DistanceTesting\\\\filenamesv2'\n",
    "fetch_and_save_postmatch_data(continent,csv_file_path, output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f52d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for MatchID: EUW1_6864052490 has been saved successfully.\n",
      "Finished processing EUW1_6864052490\n"
     ]
    }
   ],
   "source": [
    "def fetch_one_postmatch_and_save_data(continent, matchID, output_folder_path):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Charset\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "        \"Origin\": \"https://developer.riotgames.com\",\n",
    "        \"X-Riot-Token\": \"RGAPI-e75b46d3-5e51-4829-b978-68790c3ebf56\"  # Make sure to replace with your actual API token\n",
    "    }\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    # Construct the request URL\n",
    "    url = f\"https://{continent}.api.riotgames.com/lol/match/v5/matches/{matchID}\"\n",
    "    \n",
    "    # Make the request\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check the response status\n",
    "    if response.status_code == 200:\n",
    "        output_file_path = os.path.join(output_folder_path, f\"{matchID}.json\")\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(response.text)\n",
    "        print(f\"Data for MatchID: {matchID} has been saved successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for MatchID: {matchID}, Status Code: {response.status_code}\")\n",
    "\n",
    "    print(f\"Finished processing {matchID}\")\n",
    "\n",
    "# Example usage\n",
    "continent = 'europe'\n",
    "matchID = 'EUW1_6864052490'\n",
    "output_folder_path = 'E:\\\\DistanceTesting\\\\filenamesv2'\n",
    "fetch_one_postmatch_and_save_data(continent, matchID, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f86fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for MatchID: EUW1_6871911174 has been saved successfully.\n",
      "Finished processing EUW1_6871911174\n"
     ]
    }
   ],
   "source": [
    "def get_one_timeline_from_matchID(continent, matchID, output_folder_path):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Charset\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "        \"Origin\": \"https://developer.riotgames.com\",\n",
    "        \"X-Riot-Token\": \"RGAPI-e75b46d3-5e51-4829-b978-68790c3ebf56\"  # Make sure to replace with your actual API token\n",
    "    }\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    # Construct the request URL\n",
    "    url = f\"https://{continent}.api.riotgames.com/lol/match/v5/matches/{matchID}/timeline\"\n",
    "    \n",
    "    # Make the request\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check the response status\n",
    "    if response.status_code == 200:\n",
    "        output_file_path = os.path.join(output_folder_path, f\"{matchID}.json\")\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(response.text)\n",
    "        print(f\"Data for MatchID: {matchID} has been saved successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for MatchID: {matchID}, Status Code: {response.status_code}\")\n",
    "\n",
    "    print(f\"Finished processing {matchID}\")\n",
    "\n",
    "# Example usage\n",
    "continent = 'europe'\n",
    "matchID = 'EUW1_6871911174'\n",
    "output_folder_path = 'E:\\\\DistanceTesting\\\\filenamesv2'\n",
    "get_one_timeline_from_matchID(continent, matchID, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82001ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
